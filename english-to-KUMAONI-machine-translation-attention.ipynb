{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## The goal of this notebook is to implement sequence to sequence language translation (seq2seq) and Attention mechanism.\n",
    "The notebook deals with a sequence to sequence model for English to Kumaoni translation. After training the model one will be able to input a English sentence and get back its Kumaoni translation.\n",
    "\n",
    ">RNNs are also capable of doing natural language translation, aka. machine translation. It involves two RNNs, one for the source language and one for the target language. One of them is called an encoder, and the other one decoder. The reason is that, the first one encodes the sentence into a vector and the second one converts the encoded vector into a sentence in target language. The decoder is a separete RNN. Given the encoded sentence, it produces the translated sentence in target language. Attention lets the decoder to focus on specific parts of the input sentence for each output word. This helps the input and output sentences to align with one another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "PATH = \"./input/Kumaoni_English_Truncated_Corpus.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess English and Kumaoni sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    return w\n",
    "\n",
    "def kumaoni_preprocess_sentence(w):\n",
    "    w = w.rstrip().strip()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path=PATH):\n",
    "    lines=pd.read_csv(path,encoding='utf-8')\n",
    "    lines=lines.dropna()\n",
    "    lines = lines[lines['source']=='ted']\n",
    "    en = []\n",
    "    hd = []\n",
    "    for i, j in zip(lines['english_sentence'], lines['kumaoni_sentence']):\n",
    "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
    "        en_1.append('<end>')\n",
    "        en_1.insert(0, '<start>')\n",
    "        hd_1 = [kumaoni_preprocess_sentence(w) for w in j.split(' ')]\n",
    "        hd_1.append('<end>')\n",
    "        hd_1.insert(0, '<start>')\n",
    "        en.append(en_1)\n",
    "        hd.append(hd_1)\n",
    "    return hd, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path=PATH):\n",
    "    targ_lang, inp_lang = create_dataset(path)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 163 41 41\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "91 ----> fourteenth\n",
      "3 ----> \n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "264 ----> चौदहवां\n",
      "265 ----> (१४वां)\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "    \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "> We are using minimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 50\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Decoder with Attention Model\n",
    "\n",
    "> Encoder Decoder with Attention model is a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. It uses a multilayered Gated Recurrent Unit (GRU) to map the input sequence to a vector of a fixed dimensionality, and then another deep GRU to decode the target sequence from the vector.\n",
    "<img src=\"https://www.researchgate.net/profile/Vlad_Zhukov2/publication/321210603/figure/fig1/AS:642862530191361@1530281779831/An-example-of-sequence-to-sequence-model-with-attention-Calculation-of-cross-entropy.png\" width=\"800\" alt=\"attention mechanism\">\n",
    "\n",
    "> A sequence to sequence model has two parts – an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network. the task of an encoder network is to understand the input sequence, and create a smaller dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output. The input is put through an encoder model which gives us the encoder output. Here, each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. We use Bahdanau attention for the encoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "    return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#   print(type(mask))\n",
    "  loss_ *= mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    ">1. Pass *input* through *encoder* to get *encoder output*..\n",
    ">2. Then encoder output, encoder hidden state and the decoder input is passed to decoder.\n",
    ">3. Decoder returns *predictions* and *decoder hidden state*.\n",
    ">4. Decoder hidden state is then passed back to model.\n",
    ">5. Predictions are used to calculate loss.\n",
    ">6. Use *teacher forcing* (technique where the target word is passed as the next input to the decoder) for the next input to the decoder.\n",
    ">7. Calculate gradients and apply it to *optimizer* for backpropogation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    # Teacher forcing\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))      \n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.0059\n",
      "Epoch 1 Loss 0.0059\n",
      "Time taken for 1 epoch 21.099679231643677 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0059\n",
      "Epoch 2 Loss 0.0059\n",
      "Time taken for 1 epoch 0.7231674194335938 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0061\n",
      "Epoch 3 Loss 0.0058\n",
      "Time taken for 1 epoch 0.4907073974609375 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0057\n",
      "Epoch 4 Loss 0.0058\n",
      "Time taken for 1 epoch 0.6953468322753906 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0058\n",
      "Epoch 5 Loss 0.0057\n",
      "Time taken for 1 epoch 0.4935638904571533 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0056\n",
      "Epoch 6 Loss 0.0058\n",
      "Time taken for 1 epoch 0.7000601291656494 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0060\n",
      "Epoch 7 Loss 0.0058\n",
      "Time taken for 1 epoch 0.5028467178344727 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0056\n",
      "Epoch 8 Loss 0.0056\n",
      "Time taken for 1 epoch 0.61722731590271 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0060\n",
      "Epoch 9 Loss 0.0057\n",
      "Time taken for 1 epoch 0.456406831741333 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0051\n",
      "Epoch 10 Loss 0.0055\n",
      "Time taken for 1 epoch 0.6337788105010986 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0055\n",
      "Epoch 11 Loss 0.0055\n",
      "Time taken for 1 epoch 0.5116336345672607 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0056\n",
      "Epoch 12 Loss 0.0056\n",
      "Time taken for 1 epoch 0.7849864959716797 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0051\n",
      "Epoch 13 Loss 0.0056\n",
      "Time taken for 1 epoch 0.4830307960510254 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0055\n",
      "Epoch 14 Loss 0.0056\n",
      "Time taken for 1 epoch 0.6831188201904297 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0051\n",
      "Epoch 15 Loss 0.0054\n",
      "Time taken for 1 epoch 0.5123817920684814 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0051\n",
      "Epoch 16 Loss 0.0054\n",
      "Time taken for 1 epoch 0.7407064437866211 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0049\n",
      "Epoch 17 Loss 0.0053\n",
      "Time taken for 1 epoch 0.49140214920043945 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0048\n",
      "Epoch 18 Loss 0.0053\n",
      "Time taken for 1 epoch 0.6719951629638672 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0056\n",
      "Epoch 19 Loss 0.0054\n",
      "Time taken for 1 epoch 0.5047152042388916 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0051\n",
      "Epoch 20 Loss 0.0053\n",
      "Time taken for 1 epoch 0.6774682998657227 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0053\n",
      "Epoch 21 Loss 0.0052\n",
      "Time taken for 1 epoch 0.6296243667602539 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0050\n",
      "Epoch 22 Loss 0.0051\n",
      "Time taken for 1 epoch 0.7088296413421631 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0048\n",
      "Epoch 23 Loss 0.0052\n",
      "Time taken for 1 epoch 0.4862229824066162 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0053\n",
      "Epoch 24 Loss 0.0051\n",
      "Time taken for 1 epoch 0.7000076770782471 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0056\n",
      "Epoch 25 Loss 0.0050\n",
      "Time taken for 1 epoch 0.48740649223327637 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0053\n",
      "Epoch 26 Loss 0.0051\n",
      "Time taken for 1 epoch 0.6781370639801025 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0050\n",
      "Epoch 27 Loss 0.0051\n",
      "Time taken for 1 epoch 0.48708558082580566 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0050\n",
      "Epoch 28 Loss 0.0050\n",
      "Time taken for 1 epoch 0.6796462535858154 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0047\n",
      "Epoch 29 Loss 0.0050\n",
      "Time taken for 1 epoch 0.4693894386291504 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0046\n",
      "Epoch 30 Loss 0.0049\n",
      "Time taken for 1 epoch 0.6542739868164062 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0046\n",
      "Epoch 31 Loss 0.0050\n",
      "Time taken for 1 epoch 0.5704829692840576 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0051\n",
      "Epoch 32 Loss 0.0049\n",
      "Time taken for 1 epoch 0.7756273746490479 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0049\n",
      "Epoch 33 Loss 0.0049\n",
      "Time taken for 1 epoch 0.6062309741973877 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0053\n",
      "Epoch 34 Loss 0.0047\n",
      "Time taken for 1 epoch 0.6954584121704102 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0053\n",
      "Epoch 35 Loss 0.0047\n",
      "Time taken for 1 epoch 0.42714905738830566 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0044\n",
      "Epoch 36 Loss 0.0047\n",
      "Time taken for 1 epoch 0.6620776653289795 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0050\n",
      "Epoch 37 Loss 0.0048\n",
      "Time taken for 1 epoch 0.49278831481933594 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0047\n",
      "Epoch 38 Loss 0.0047\n",
      "Time taken for 1 epoch 0.7090940475463867 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0046\n",
      "Epoch 39 Loss 0.0047\n",
      "Time taken for 1 epoch 0.5076451301574707 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0045\n",
      "Epoch 40 Loss 0.0047\n",
      "Time taken for 1 epoch 0.6951756477355957 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0044\n",
      "Epoch 41 Loss 0.0047\n",
      "Time taken for 1 epoch 0.49613046646118164 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0044\n",
      "Epoch 42 Loss 0.0047\n",
      "Time taken for 1 epoch 0.6765270233154297 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0047\n",
      "Epoch 43 Loss 0.0046\n",
      "Time taken for 1 epoch 0.5022192001342773 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0044\n",
      "Epoch 44 Loss 0.0046\n",
      "Time taken for 1 epoch 0.6622662544250488 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0043\n",
      "Epoch 45 Loss 0.0045\n",
      "Time taken for 1 epoch 0.47975921630859375 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0047\n",
      "Epoch 46 Loss 0.0045\n",
      "Time taken for 1 epoch 0.6354055404663086 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0043\n",
      "Epoch 47 Loss 0.0045\n",
      "Time taken for 1 epoch 0.46817636489868164 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0047\n",
      "Epoch 48 Loss 0.0045\n",
      "Time taken for 1 epoch 0.6128559112548828 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0044\n",
      "Epoch 49 Loss 0.0045\n",
      "Time taken for 1 epoch 0.4470949172973633 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0044\n",
      "Epoch 50 Loss 0.0044\n",
      "Time taken for 1 epoch 0.6613585948944092 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0042\n",
      "Epoch 51 Loss 0.0044\n",
      "Time taken for 1 epoch 0.5241482257843018 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0040\n",
      "Epoch 52 Loss 0.0043\n",
      "Time taken for 1 epoch 0.6977484226226807 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0043\n",
      "Epoch 53 Loss 0.0044\n",
      "Time taken for 1 epoch 0.5014126300811768 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.0040\n",
      "Epoch 54 Loss 0.0043\n",
      "Time taken for 1 epoch 0.6868588924407959 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0046\n",
      "Epoch 55 Loss 0.0042\n",
      "Time taken for 1 epoch 0.4656236171722412 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0041\n",
      "Epoch 56 Loss 0.0042\n",
      "Time taken for 1 epoch 0.6856377124786377 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0042\n",
      "Epoch 57 Loss 0.0042\n",
      "Time taken for 1 epoch 0.4774477481842041 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0041\n",
      "Epoch 58 Loss 0.0042\n",
      "Time taken for 1 epoch 0.6627087593078613 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0042\n",
      "Epoch 59 Loss 0.0041\n",
      "Time taken for 1 epoch 0.4787452220916748 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0037\n",
      "Epoch 60 Loss 0.0042\n",
      "Time taken for 1 epoch 0.6800658702850342 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0040\n",
      "Epoch 61 Loss 0.0041\n",
      "Time taken for 1 epoch 0.4797234535217285 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0039\n",
      "Epoch 62 Loss 0.0040\n",
      "Time taken for 1 epoch 0.6508123874664307 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0045\n",
      "Epoch 63 Loss 0.0041\n",
      "Time taken for 1 epoch 0.46620965003967285 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0044\n",
      "Epoch 64 Loss 0.0040\n",
      "Time taken for 1 epoch 0.6585581302642822 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0038\n",
      "Epoch 65 Loss 0.0040\n",
      "Time taken for 1 epoch 0.5143215656280518 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0038\n",
      "Epoch 66 Loss 0.0040\n",
      "Time taken for 1 epoch 0.6763026714324951 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0046\n",
      "Epoch 67 Loss 0.0041\n",
      "Time taken for 1 epoch 0.45563721656799316 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.0039\n",
      "Epoch 68 Loss 0.0040\n",
      "Time taken for 1 epoch 0.7559599876403809 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0045\n",
      "Epoch 69 Loss 0.0040\n",
      "Time taken for 1 epoch 0.5078067779541016 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0038\n",
      "Epoch 70 Loss 0.0039\n",
      "Time taken for 1 epoch 0.7273077964782715 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.0040\n",
      "Epoch 71 Loss 0.0039\n",
      "Time taken for 1 epoch 0.5101113319396973 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.0038\n",
      "Epoch 72 Loss 0.0038\n",
      "Time taken for 1 epoch 0.750901460647583 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.0038\n",
      "Epoch 73 Loss 0.0038\n",
      "Time taken for 1 epoch 0.5516524314880371 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.0037\n",
      "Epoch 74 Loss 0.0038\n",
      "Time taken for 1 epoch 0.7203819751739502 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.0040\n",
      "Epoch 75 Loss 0.0038\n",
      "Time taken for 1 epoch 0.5106046199798584 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.0036\n",
      "Epoch 76 Loss 0.0038\n",
      "Time taken for 1 epoch 0.7198476791381836 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.0039\n",
      "Epoch 77 Loss 0.0038\n",
      "Time taken for 1 epoch 0.48778796195983887 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.0035\n",
      "Epoch 78 Loss 0.0037\n",
      "Time taken for 1 epoch 0.6602275371551514 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.0037\n",
      "Epoch 79 Loss 0.0037\n",
      "Time taken for 1 epoch 0.4621763229370117 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.0039\n",
      "Epoch 80 Loss 0.0038\n",
      "Time taken for 1 epoch 0.6408514976501465 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.0036\n",
      "Epoch 81 Loss 0.0037\n",
      "Time taken for 1 epoch 0.455263614654541 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.0036\n",
      "Epoch 82 Loss 0.0037\n",
      "Time taken for 1 epoch 0.6025040149688721 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.0037\n",
      "Epoch 83 Loss 0.0037\n",
      "Time taken for 1 epoch 0.4265592098236084 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.0035\n",
      "Epoch 84 Loss 0.0037\n",
      "Time taken for 1 epoch 0.6709041595458984 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.0037\n",
      "Epoch 85 Loss 0.0036\n",
      "Time taken for 1 epoch 0.48607635498046875 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.0035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Loss 0.0037\n",
      "Time taken for 1 epoch 0.5722551345825195 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.0035\n",
      "Epoch 87 Loss 0.0036\n",
      "Time taken for 1 epoch 0.4267454147338867 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.0038\n",
      "Epoch 88 Loss 0.0036\n",
      "Time taken for 1 epoch 0.6163623332977295 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.0032\n",
      "Epoch 89 Loss 0.0036\n",
      "Time taken for 1 epoch 0.49675989151000977 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.0034\n",
      "Epoch 90 Loss 0.0036\n",
      "Time taken for 1 epoch 0.688096284866333 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.0037\n",
      "Epoch 91 Loss 0.0035\n",
      "Time taken for 1 epoch 0.48198509216308594 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.0036\n",
      "Epoch 92 Loss 0.0035\n",
      "Time taken for 1 epoch 0.7003202438354492 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.0034\n",
      "Epoch 93 Loss 0.0035\n",
      "Time taken for 1 epoch 0.4943268299102783 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.0034\n",
      "Epoch 94 Loss 0.0034\n",
      "Time taken for 1 epoch 0.772794246673584 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.0037\n",
      "Epoch 95 Loss 0.0035\n",
      "Time taken for 1 epoch 0.5404987335205078 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.0037\n",
      "Epoch 96 Loss 0.0034\n",
      "Time taken for 1 epoch 0.6513643264770508 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.0036\n",
      "Epoch 97 Loss 0.0034\n",
      "Time taken for 1 epoch 0.48532772064208984 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.0035\n",
      "Epoch 98 Loss 0.0034\n",
      "Time taken for 1 epoch 0.6804099082946777 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.0034\n",
      "Epoch 99 Loss 0.0034\n",
      "Time taken for 1 epoch 0.51218581199646 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.0032\n",
      "Epoch 100 Loss 0.0033\n",
      "Time taken for 1 epoch 0.6882092952728271 sec\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.0031\n",
      "Epoch 101 Loss 0.0034\n",
      "Time taken for 1 epoch 0.5004475116729736 sec\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.0031\n",
      "Epoch 102 Loss 0.0033\n",
      "Time taken for 1 epoch 0.6912078857421875 sec\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.0034\n",
      "Epoch 103 Loss 0.0033\n",
      "Time taken for 1 epoch 0.5071690082550049 sec\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.0036\n",
      "Epoch 104 Loss 0.0033\n",
      "Time taken for 1 epoch 0.7029619216918945 sec\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.0031\n",
      "Epoch 105 Loss 0.0033\n",
      "Time taken for 1 epoch 0.5110187530517578 sec\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.0033\n",
      "Epoch 106 Loss 0.0033\n",
      "Time taken for 1 epoch 0.7385087013244629 sec\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.0033\n",
      "Epoch 107 Loss 0.0032\n",
      "Time taken for 1 epoch 0.5211515426635742 sec\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.0032\n",
      "Epoch 108 Loss 0.0032\n",
      "Time taken for 1 epoch 0.7135310173034668 sec\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.0033\n",
      "Epoch 109 Loss 0.0032\n",
      "Time taken for 1 epoch 0.4827446937561035 sec\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.0034\n",
      "Epoch 110 Loss 0.0032\n",
      "Time taken for 1 epoch 0.7109532356262207 sec\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.0032\n",
      "Epoch 111 Loss 0.0032\n",
      "Time taken for 1 epoch 0.49520254135131836 sec\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.0032\n",
      "Epoch 112 Loss 0.0032\n",
      "Time taken for 1 epoch 0.6979737281799316 sec\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.0031\n",
      "Epoch 113 Loss 0.0031\n",
      "Time taken for 1 epoch 0.5039591789245605 sec\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.0032\n",
      "Epoch 114 Loss 0.0032\n",
      "Time taken for 1 epoch 0.6954410076141357 sec\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.0030\n",
      "Epoch 115 Loss 0.0031\n",
      "Time taken for 1 epoch 0.48316335678100586 sec\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.0032\n",
      "Epoch 116 Loss 0.0031\n",
      "Time taken for 1 epoch 0.6676187515258789 sec\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.0031\n",
      "Epoch 117 Loss 0.0031\n",
      "Time taken for 1 epoch 0.28173208236694336 sec\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.0035\n",
      "Epoch 118 Loss 0.0031\n",
      "Time taken for 1 epoch 0.37515807151794434 sec\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.0030\n",
      "Epoch 119 Loss 0.0031\n",
      "Time taken for 1 epoch 0.2683248519897461 sec\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.0029\n",
      "Epoch 120 Loss 0.0031\n",
      "Time taken for 1 epoch 0.36568117141723633 sec\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.0030\n",
      "Epoch 121 Loss 0.0030\n",
      "Time taken for 1 epoch 0.25589871406555176 sec\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.0029\n",
      "Epoch 122 Loss 0.0030\n",
      "Time taken for 1 epoch 0.3351907730102539 sec\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.0032\n",
      "Epoch 123 Loss 0.0030\n",
      "Time taken for 1 epoch 0.2488548755645752 sec\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.0031\n",
      "Epoch 124 Loss 0.0030\n",
      "Time taken for 1 epoch 0.34729981422424316 sec\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.0030\n",
      "Epoch 125 Loss 0.0030\n",
      "Time taken for 1 epoch 0.2704041004180908 sec\n",
      "\n",
      "Epoch 126 Batch 0 Loss 0.0029\n",
      "Epoch 126 Loss 0.0030\n",
      "Time taken for 1 epoch 0.36507105827331543 sec\n",
      "\n",
      "Epoch 127 Batch 0 Loss 0.0031\n",
      "Epoch 127 Loss 0.0030\n",
      "Time taken for 1 epoch 0.24979281425476074 sec\n",
      "\n",
      "Epoch 128 Batch 0 Loss 0.0029\n",
      "Epoch 128 Loss 0.0030\n",
      "Time taken for 1 epoch 0.3373730182647705 sec\n",
      "\n",
      "Epoch 129 Batch 0 Loss 0.0031\n",
      "Epoch 129 Loss 0.0029\n",
      "Time taken for 1 epoch 0.24831938743591309 sec\n",
      "\n",
      "Epoch 130 Batch 0 Loss 0.0028\n",
      "Epoch 130 Loss 0.0029\n",
      "Time taken for 1 epoch 0.33240509033203125 sec\n",
      "\n",
      "Epoch 131 Batch 0 Loss 0.0026\n",
      "Epoch 131 Loss 0.0029\n",
      "Time taken for 1 epoch 0.24410486221313477 sec\n",
      "\n",
      "Epoch 132 Batch 0 Loss 0.0031\n",
      "Epoch 132 Loss 0.0029\n",
      "Time taken for 1 epoch 0.33537793159484863 sec\n",
      "\n",
      "Epoch 133 Batch 0 Loss 0.0028\n",
      "Epoch 133 Loss 0.0029\n",
      "Time taken for 1 epoch 0.25113463401794434 sec\n",
      "\n",
      "Epoch 134 Batch 0 Loss 0.0027\n",
      "Epoch 134 Loss 0.0028\n",
      "Time taken for 1 epoch 0.33208417892456055 sec\n",
      "\n",
      "Epoch 135 Batch 0 Loss 0.0029\n",
      "Epoch 135 Loss 0.0028\n",
      "Time taken for 1 epoch 0.2464001178741455 sec\n",
      "\n",
      "Epoch 136 Batch 0 Loss 0.0028\n",
      "Epoch 136 Loss 0.0029\n",
      "Time taken for 1 epoch 0.3490486145019531 sec\n",
      "\n",
      "Epoch 137 Batch 0 Loss 0.0029\n",
      "Epoch 137 Loss 0.0028\n",
      "Time taken for 1 epoch 0.24616765975952148 sec\n",
      "\n",
      "Epoch 138 Batch 0 Loss 0.0028\n",
      "Epoch 138 Loss 0.0028\n",
      "Time taken for 1 epoch 0.36046266555786133 sec\n",
      "\n",
      "Epoch 139 Batch 0 Loss 0.0028\n",
      "Epoch 139 Loss 0.0028\n",
      "Time taken for 1 epoch 0.2543754577636719 sec\n",
      "\n",
      "Epoch 140 Batch 0 Loss 0.0026\n",
      "Epoch 140 Loss 0.0028\n",
      "Time taken for 1 epoch 0.40268540382385254 sec\n",
      "\n",
      "Epoch 141 Batch 0 Loss 0.0026\n",
      "Epoch 141 Loss 0.0028\n",
      "Time taken for 1 epoch 0.25215935707092285 sec\n",
      "\n",
      "Epoch 142 Batch 0 Loss 0.0027\n",
      "Epoch 142 Loss 0.0027\n",
      "Time taken for 1 epoch 0.38140225410461426 sec\n",
      "\n",
      "Epoch 143 Batch 0 Loss 0.0030\n",
      "Epoch 143 Loss 0.0028\n",
      "Time taken for 1 epoch 0.26412248611450195 sec\n",
      "\n",
      "Epoch 144 Batch 0 Loss 0.0028\n",
      "Epoch 144 Loss 0.0027\n",
      "Time taken for 1 epoch 0.37287211418151855 sec\n",
      "\n",
      "Epoch 145 Batch 0 Loss 0.0029\n",
      "Epoch 145 Loss 0.0027\n",
      "Time taken for 1 epoch 0.2618579864501953 sec\n",
      "\n",
      "Epoch 146 Batch 0 Loss 0.0028\n",
      "Epoch 146 Loss 0.0027\n",
      "Time taken for 1 epoch 0.3911731243133545 sec\n",
      "\n",
      "Epoch 147 Batch 0 Loss 0.0027\n",
      "Epoch 147 Loss 0.0027\n",
      "Time taken for 1 epoch 0.25390172004699707 sec\n",
      "\n",
      "Epoch 148 Batch 0 Loss 0.0028\n",
      "Epoch 148 Loss 0.0026\n",
      "Time taken for 1 epoch 0.38322997093200684 sec\n",
      "\n",
      "Epoch 149 Batch 0 Loss 0.0027\n",
      "Epoch 149 Loss 0.0027\n",
      "Time taken for 1 epoch 0.2728848457336426 sec\n",
      "\n",
      "Epoch 150 Batch 0 Loss 0.0024\n",
      "Epoch 150 Loss 0.0026\n",
      "Time taken for 1 epoch 0.388690710067749 sec\n",
      "\n",
      "Epoch 151 Batch 0 Loss 0.0028\n",
      "Epoch 151 Loss 0.0027\n",
      "Time taken for 1 epoch 0.26538753509521484 sec\n",
      "\n",
      "Epoch 152 Batch 0 Loss 0.0025\n",
      "Epoch 152 Loss 0.0026\n",
      "Time taken for 1 epoch 0.3799421787261963 sec\n",
      "\n",
      "Epoch 153 Batch 0 Loss 0.0025\n",
      "Epoch 153 Loss 0.0027\n",
      "Time taken for 1 epoch 0.24988102912902832 sec\n",
      "\n",
      "Epoch 154 Batch 0 Loss 0.0025\n",
      "Epoch 154 Loss 0.0026\n",
      "Time taken for 1 epoch 0.39017605781555176 sec\n",
      "\n",
      "Epoch 155 Batch 0 Loss 0.0026\n",
      "Epoch 155 Loss 0.0026\n",
      "Time taken for 1 epoch 0.2500648498535156 sec\n",
      "\n",
      "Epoch 156 Batch 0 Loss 0.0026\n",
      "Epoch 156 Loss 0.0026\n",
      "Time taken for 1 epoch 0.3495666980743408 sec\n",
      "\n",
      "Epoch 157 Batch 0 Loss 0.0028\n",
      "Epoch 157 Loss 0.0026\n",
      "Time taken for 1 epoch 0.2578916549682617 sec\n",
      "\n",
      "Epoch 158 Batch 0 Loss 0.0026\n",
      "Epoch 158 Loss 0.0026\n",
      "Time taken for 1 epoch 0.3442840576171875 sec\n",
      "\n",
      "Epoch 159 Batch 0 Loss 0.0025\n",
      "Epoch 159 Loss 0.0026\n",
      "Time taken for 1 epoch 0.2781834602355957 sec\n",
      "\n",
      "Epoch 160 Batch 0 Loss 0.0024\n",
      "Epoch 160 Loss 0.0025\n",
      "Time taken for 1 epoch 0.3597855567932129 sec\n",
      "\n",
      "Epoch 161 Batch 0 Loss 0.0026\n",
      "Epoch 161 Loss 0.0025\n",
      "Time taken for 1 epoch 0.26917529106140137 sec\n",
      "\n",
      "Epoch 162 Batch 0 Loss 0.0024\n",
      "Epoch 162 Loss 0.0026\n",
      "Time taken for 1 epoch 0.36090612411499023 sec\n",
      "\n",
      "Epoch 163 Batch 0 Loss 0.0025\n",
      "Epoch 163 Loss 0.0026\n",
      "Time taken for 1 epoch 0.26975083351135254 sec\n",
      "\n",
      "Epoch 164 Batch 0 Loss 0.0026\n",
      "Epoch 164 Loss 0.0026\n",
      "Time taken for 1 epoch 0.38266491889953613 sec\n",
      "\n",
      "Epoch 165 Batch 0 Loss 0.0025\n",
      "Epoch 165 Loss 0.0025\n",
      "Time taken for 1 epoch 0.292111873626709 sec\n",
      "\n",
      "Epoch 166 Batch 0 Loss 0.0026\n",
      "Epoch 166 Loss 0.0025\n",
      "Time taken for 1 epoch 0.39560890197753906 sec\n",
      "\n",
      "Epoch 167 Batch 0 Loss 0.0025\n",
      "Epoch 167 Loss 0.0025\n",
      "Time taken for 1 epoch 0.28139615058898926 sec\n",
      "\n",
      "Epoch 168 Batch 0 Loss 0.0024\n",
      "Epoch 168 Loss 0.0024\n",
      "Time taken for 1 epoch 0.4130361080169678 sec\n",
      "\n",
      "Epoch 169 Batch 0 Loss 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169 Loss 0.0025\n",
      "Time taken for 1 epoch 0.2853519916534424 sec\n",
      "\n",
      "Epoch 170 Batch 0 Loss 0.0025\n",
      "Epoch 170 Loss 0.0024\n",
      "Time taken for 1 epoch 0.4200439453125 sec\n",
      "\n",
      "Epoch 171 Batch 0 Loss 0.0026\n",
      "Epoch 171 Loss 0.0025\n",
      "Time taken for 1 epoch 0.2900669574737549 sec\n",
      "\n",
      "Epoch 172 Batch 0 Loss 0.0022\n",
      "Epoch 172 Loss 0.0024\n",
      "Time taken for 1 epoch 0.4407925605773926 sec\n",
      "\n",
      "Epoch 173 Batch 0 Loss 0.0023\n",
      "Epoch 173 Loss 0.0024\n",
      "Time taken for 1 epoch 0.3325319290161133 sec\n",
      "\n",
      "Epoch 174 Batch 0 Loss 0.0023\n",
      "Epoch 174 Loss 0.0024\n",
      "Time taken for 1 epoch 0.4370126724243164 sec\n",
      "\n",
      "Epoch 175 Batch 0 Loss 0.0025\n",
      "Epoch 175 Loss 0.0024\n",
      "Time taken for 1 epoch 0.35398244857788086 sec\n",
      "\n",
      "Epoch 176 Batch 0 Loss 0.0022\n",
      "Epoch 176 Loss 0.0024\n",
      "Time taken for 1 epoch 0.5156879425048828 sec\n",
      "\n",
      "Epoch 177 Batch 0 Loss 0.0024\n",
      "Epoch 177 Loss 0.0024\n",
      "Time taken for 1 epoch 0.29281187057495117 sec\n",
      "\n",
      "Epoch 178 Batch 0 Loss 0.0026\n",
      "Epoch 178 Loss 0.0024\n",
      "Time taken for 1 epoch 0.5360145568847656 sec\n",
      "\n",
      "Epoch 179 Batch 0 Loss 0.0022\n",
      "Epoch 179 Loss 0.0024\n",
      "Time taken for 1 epoch 0.2931389808654785 sec\n",
      "\n",
      "Epoch 180 Batch 0 Loss 0.0022\n",
      "Epoch 180 Loss 0.0023\n",
      "Time taken for 1 epoch 0.4537813663482666 sec\n",
      "\n",
      "Epoch 181 Batch 0 Loss 0.0026\n",
      "Epoch 181 Loss 0.0024\n",
      "Time taken for 1 epoch 0.32899022102355957 sec\n",
      "\n",
      "Epoch 182 Batch 0 Loss 0.0024\n",
      "Epoch 182 Loss 0.0023\n",
      "Time taken for 1 epoch 0.4669992923736572 sec\n",
      "\n",
      "Epoch 183 Batch 0 Loss 0.0023\n",
      "Epoch 183 Loss 0.0023\n",
      "Time taken for 1 epoch 0.2846388816833496 sec\n",
      "\n",
      "Epoch 184 Batch 0 Loss 0.0024\n",
      "Epoch 184 Loss 0.0023\n",
      "Time taken for 1 epoch 0.46845102310180664 sec\n",
      "\n",
      "Epoch 185 Batch 0 Loss 0.0023\n",
      "Epoch 185 Loss 0.0023\n",
      "Time taken for 1 epoch 0.3052704334259033 sec\n",
      "\n",
      "Epoch 186 Batch 0 Loss 0.0022\n",
      "Epoch 186 Loss 0.0023\n",
      "Time taken for 1 epoch 0.414736270904541 sec\n",
      "\n",
      "Epoch 187 Batch 0 Loss 0.0023\n",
      "Epoch 187 Loss 0.0023\n",
      "Time taken for 1 epoch 0.27606964111328125 sec\n",
      "\n",
      "Epoch 188 Batch 0 Loss 0.0025\n",
      "Epoch 188 Loss 0.0023\n",
      "Time taken for 1 epoch 0.4023556709289551 sec\n",
      "\n",
      "Epoch 189 Batch 0 Loss 0.0024\n",
      "Epoch 189 Loss 0.0023\n",
      "Time taken for 1 epoch 0.29241299629211426 sec\n",
      "\n",
      "Epoch 190 Batch 0 Loss 0.0021\n",
      "Epoch 190 Loss 0.0022\n",
      "Time taken for 1 epoch 0.369215726852417 sec\n",
      "\n",
      "Epoch 191 Batch 0 Loss 0.0023\n",
      "Epoch 191 Loss 0.0022\n",
      "Time taken for 1 epoch 0.27791452407836914 sec\n",
      "\n",
      "Epoch 192 Batch 0 Loss 0.0023\n",
      "Epoch 192 Loss 0.0022\n",
      "Time taken for 1 epoch 0.3784489631652832 sec\n",
      "\n",
      "Epoch 193 Batch 0 Loss 0.0022\n",
      "Epoch 193 Loss 0.0023\n",
      "Time taken for 1 epoch 0.27860498428344727 sec\n",
      "\n",
      "Epoch 194 Batch 0 Loss 0.0021\n",
      "Epoch 194 Loss 0.0022\n",
      "Time taken for 1 epoch 0.37656354904174805 sec\n",
      "\n",
      "Epoch 195 Batch 0 Loss 0.0023\n",
      "Epoch 195 Loss 0.0022\n",
      "Time taken for 1 epoch 0.27854013442993164 sec\n",
      "\n",
      "Epoch 196 Batch 0 Loss 0.0021\n",
      "Epoch 196 Loss 0.0022\n",
      "Time taken for 1 epoch 0.38006067276000977 sec\n",
      "\n",
      "Epoch 197 Batch 0 Loss 0.0024\n",
      "Epoch 197 Loss 0.0022\n",
      "Time taken for 1 epoch 0.2907099723815918 sec\n",
      "\n",
      "Epoch 198 Batch 0 Loss 0.0021\n",
      "Epoch 198 Loss 0.0022\n",
      "Time taken for 1 epoch 0.37976980209350586 sec\n",
      "\n",
      "Epoch 199 Batch 0 Loss 0.0023\n",
      "Epoch 199 Loss 0.0022\n",
      "Time taken for 1 epoch 0.277651309967041 sec\n",
      "\n",
      "Epoch 200 Batch 0 Loss 0.0024\n",
      "Epoch 200 Loss 0.0021\n",
      "Time taken for 1 epoch 0.38451266288757324 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x21744863688>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: first\n",
      "Predicted translation: hituul <end> \n"
     ]
    }
   ],
   "source": [
    "translate('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: one\n",
      "Predicted translation: khulul <end> \n"
     ]
    }
   ],
   "source": [
    "translate('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: how are you\n",
      "Predicted translation: uttar kei houl? <end> \n"
     ]
    }
   ],
   "source": [
    "translate('how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: i\n",
      "Predicted translation: yo <end> \n"
     ]
    }
   ],
   "source": [
    "translate('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: she\n",
      "Predicted translation: yo <end> \n"
     ]
    }
   ],
   "source": [
    "translate('she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: why did you tell him to go\n",
      "Predicted translation: uul <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Why did you tell him to go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: nine\n",
      "Predicted translation: तीसरा/तृतीय (३रा/३य) <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Nine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: how many apples are there\n",
      "Predicted translation: prasan/sawaal key chin? <end> \n"
     ]
    }
   ],
   "source": [
    "translate('How many apples are there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: why are they late for the\n",
      "Predicted translation: tull gadi kasi chalaei? <end> \n"
     ]
    }
   ],
   "source": [
    "translate('why are they late for the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: will you come with me\n",
      "Predicted translation: key sawaal chi? <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Will you come with me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: i shall come with you\n",
      "Predicted translation: janai <end> \n"
     ]
    }
   ],
   "source": [
    "translate('I shall come with you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: he will go\n",
      "Predicted translation: yo <end> \n"
     ]
    }
   ],
   "source": [
    "translate('He will go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: how are you\n",
      "Predicted translation: uttar kei houl? <end> \n"
     ]
    }
   ],
   "source": [
    "translate('how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-9b50de7241bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'It is the answer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-78-4c360bb459a8>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted translation: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-0ffe3188733a>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mattention_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_length_targ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length_inp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[0;32m      6\u001b[0m                                                            \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-0ffe3188733a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mattention_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_length_targ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length_inp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[0;32m      6\u001b[0m                                                            \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'answer'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
